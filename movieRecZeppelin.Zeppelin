{"paragraphs":[{"text":"import org.apache.spark.ml.recommendation.ALS\nimport org.apache.spark.sql.Dataset\nimport spark.implicits._\n\nval sqlContext = new org.apache.spark.sql.SQLContext(sc)\nval dataDir = \"s3a://ace-useast1-users/marko/\"","dateUpdated":"2017-01-18T18:17:36+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":true,"editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1484684526772_-1903269345","id":"20170117-202206_2111014669","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.ml.recommendation.ALS\n\nimport org.apache.spark.sql.Dataset\n\nimport spark.implicits._\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@317318da\n\ndataDir: String = s3a://ace-useast1-users/marko/\n"},"dateCreated":"2017-01-17T20:22:06+0000","dateStarted":"2017-01-18T18:17:36+0000","dateFinished":"2017-01-18T18:18:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2888"},{"text":"// import data and split into train/validation/test\ncase class Rating(userId: Int, movieId: Int, rating: Float, timestamp: Long)\nval ratings = sqlContext.read.textFile(dataDir + \"/ratings.dat\").map(_.split(\"::\")).map(x => Rating(x(0).toInt, x(1).toInt, x(2).toFloat, x(3).toLong))\nval Array(training, validation, test) = ratings.randomSplit(Array(0.6, 0.2, 0.2))","dateUpdated":"2017-01-18T18:18:19+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1484749284867_1141003220","id":"20170118-142124_1873532719","result":{"code":"SUCCESS","type":"TEXT","msg":"\ndefined class Rating\n\nratings: org.apache.spark.sql.Dataset[Rating] = [userId: int, movieId: int ... 2 more fields]\n\n\n\ntraining: org.apache.spark.sql.Dataset[Rating] = [userId: int, movieId: int ... 2 more fields]\nvalidation: org.apache.spark.sql.Dataset[Rating] = [userId: int, movieId: int ... 2 more fields]\ntest: org.apache.spark.sql.Dataset[Rating] = [userId: int, movieId: int ... 2 more fields]\n"},"dateCreated":"2017-01-18T14:21:24+0000","dateStarted":"2017-01-18T18:18:20+0000","dateFinished":"2017-01-18T18:18:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2889"},{"text":"// define hyperparameter search space\nval ranks : List[Int] = List(3, 10, 30, 100)\nval numIters : List[Int] = List(10, 30)\nval lambdas : List[Double] = List(0.0001, 0.001, 0.01, 0.1)\nval alphas : List[Double] = List(0,0.1,0.3,1)\n\n//generate all possible combinations of hyperparameters\ncase class Hyperparams(rank: Int, numIter: Int, lambda: Double, alpha: Double)\nval combos : List[Hyperparams] = ranks.flatMap(x => numIters.flatMap(y => lambdas.flatMap(z => alphas.map(a => Hyperparams(x,y,z,a)))))","dateUpdated":"2017-01-18T18:18:37+0000","config":{"colWidth":12,"graph":{"mode":"table","height":93,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1484749150217_-380476176","id":"20170118-141910_869762499","result":{"code":"SUCCESS","type":"TEXT","msg":"\nranks: List[Int] = List(3, 10, 30, 100)\n\nnumIters: List[Int] = List(10, 30)\n\nlambdas: List[Double] = List(1.0E-4, 0.001, 0.01, 0.1)\n\nalphas: List[Double] = List(0.0, 0.1, 0.3, 1.0)\n\ndefined class Hyperparams\ncombos: List[Hyperparams] = List(Hyperparams(3,10,1.0E-4,0.0), Hyperparams(3,10,1.0E-4,0.1), Hyperparams(3,10,1.0E-4,0.3), Hyperparams(3,10,1.0E-4,1.0), Hyperparams(3,10,0.001,0.0), Hyperparams(3,10,0.001,0.1), Hyperparams(3,10,0.001,0.3), Hyperparams(3,10,0.001,1.0), Hyperparams(3,10,0.01,0.0), Hyperparams(3,10,0.01,0.1), Hyperparams(3,10,0.01,0.3), Hyperparams(3,10,0.01,1.0), Hyperparams(3,10,0.1,0.0), Hyperparams(3,10,0.1,0.1), Hyperparams(3,10,0.1,0.3), Hyperparams(3,10,0.1,1.0), Hyperparams(3,30,1.0E-4,0.0), Hyperparams(3,30,1.0E-4,0.1), Hyperparams(3,30,1.0E-4,0.3), Hyperparams(3,30,1.0E-4,1.0), Hyperparams(3,30,0.001,0.0), Hyperparams(3,30,0.001,0.1), Hyperparams(3,30,0.001,0.3), Hyperparams(3,30,0.001,1.0), Hyperparams(3,30,0.01,0.0), Hyperparams(3,30,0.01,0.1), Hyperparams(3,30..."},"dateCreated":"2017-01-18T14:19:10+0000","dateStarted":"2017-01-18T18:18:37+0000","dateFinished":"2017-01-18T18:18:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2890"},{"text":"// Prediction and Model Output case classes\ncase class Pred(userID: Int, movieId: Int, rating: Double, timestamp: Int, prediction: Double)\ncase class ModelOutputs(model: org.apache.spark.ml.recommendation.ALSModel, validationRmse: Float, \n                        rank: Int, lambda: Double, numIter: Int, alpha: Double)\n\n// rmse calculation that drops NaN prediction values (built-in RegressionEvaluator class cannot deal with NaN's)\n// NaN prediction values arise when a user/item combo is not calculable due to no collaborative data\ndef rmse(ds: org.apache.spark.sql.Dataset[Pred]): Float = {\n    val mse = ds.map(x => math.pow((x.rating - x.prediction),2)).filter(!_.isNaN).reduce(_+_)/\n        (ds.filter(!_.prediction.isNaN).count)\n    math.sqrt(mse).toFloat\n}","dateUpdated":"2017-01-18T18:14:39+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1484749152475_-1569719702","id":"20170118-141912_144842719","result":{"code":"SUCCESS","type":"TEXT","msg":"\ndefined class Pred\n\ndefined class ModelOutputs\n\nrmse: (ds: org.apache.spark.sql.Dataset[Pred])Float\n"},"dateCreated":"2017-01-18T14:19:12+0000","dateStarted":"2017-01-18T18:15:01+0000","dateFinished":"2017-01-18T18:15:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2891"},{"text":"// hyperparameter search function\n\ndef hyperParamSearch(combos: List[Hyperparams], training: Dataset[Rating],\n                     validation: Dataset[Rating], bestValidationRmse: Float = Float.MaxValue,\n                     returnList : List[ModelOutputs] = List()) : ModelOutputs = {\n\n    // private func to allow tail recursion and prevent stack overflows\n    @scala.annotation.tailrec\n    def hyperParamSearchf(combos: List[Hyperparams], training: Dataset[Rating],\n                            validation: Dataset[Rating], bestValidationRmse: Float,\n                            returnList: List[ModelOutputs]): ModelOutputs = {\n\n    val head = combos.head\n    val als = new ALS().\n        setMaxIter(head.numIter).\n        setRegParam(head.lambda).\n        setAlpha(head.alpha).\n        setRank(head.rank).\n        setUserCol(\"userId\").\n        setItemCol(\"movieId\").\n        setRatingCol(\"rating\")\n    val model = als.fit(training)\n    \n    val predictions = model.transform(validation).\n        map(x => Pred((0).toInt,(1).toInt,(2).toDouble,(3).toInt,x(4).toString.toDouble))\n    val validationRmse = rmse(predictions)\n\n    println(\"RMSE (validation) = \" + validationRmse + \" for the model trained with rank = \"\n      + head.rank + \", lambda = \" + head.lambda + \", alpha = \" + head.alpha + \" and numIter = \" + head.numIter + \".\")\n\n    if (combos.tail.isEmpty) {\n      returnList.minBy(_.validationRmse)\n    }\n    else {\n      val updatedReturnList = ModelOutputs(model, validationRmse, head.rank, head.lambda, head.numIter, head.alpha) :: returnList\n      hyperParamSearchf(combos.tail, training, validation, validationRmse, updatedReturnList)\n    }\n  }\n  hyperParamSearchf(combos, training, validation, bestValidationRmse, returnList)\n}","dateUpdated":"2017-01-18T18:14:39+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1484760464879_-1944452417","id":"20170118-172744_764918025","result":{"code":"ERROR","type":"TEXT","msg":"\n\n\n<console>:73: error: not found: type Hyperparams\n       def hyperParamSearch(combos: List[Hyperparams], training: Dataset[Rating],\n                                         ^\n\n\n\n<console>:73: error: not found: type Rating\n       def hyperParamSearch(combos: List[Hyperparams], training: Dataset[Rating],\n                                                                         ^\n\n\n\n<console>:74: error: not found: type Rating\n                            validation: Dataset[Rating], bestValidationRmse: Float = Float.MaxValue,\n                                                ^\n\n\n\n<console>:79: error: not found: type Hyperparams\n           def hyperParamSearchf(combos: List[Hyperparams], training: Dataset[Rating],\n                                              ^\n\n\n\n<console>:79: error: not found: type Rating\n           def hyperParamSearchf(combos: List[Hyperparams], training: Dataset[Rating],\n                                                                              ^\n\n\n\n<console>:80: error: not found: type Rating\n                                   validation: Dataset[Rating], bestValidationRmse: Float,\n                                                       ^\n"},"dateCreated":"2017-01-18T17:27:44+0000","dateStarted":"2017-01-18T18:15:12+0000","dateFinished":"2017-01-18T18:15:14+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:2892"},{"text":"val bestSettings = hyperParamSearch(combos, training, validation)","dateUpdated":"2017-01-18T18:14:39+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1484750127731_-907121691","id":"20170118-143527_787947540","result":{"code":"ERROR","type":"TEXT","msg":"RMSE (validation) = 1.6831559 for the model trained with rank = 3, lambda = 1.0E-4, alpha = 0.0 and numIter = 10.\nRMSE (validation) = 1.6831559 for the model trained with rank = 3, lambda = 1.0E-4, alpha = 0.1 and numIter = 10.\nRMSE (validation) = 1.6831559 for the model trained with rank = 3, lambda = 1.0E-4, alpha = 0.3 and numIter = 10.\nRMSE (validation) = 1.6831559 for the model trained with rank = 3, lambda = 1.0E-4, alpha = 1.0 and numIter = 10.\nRMSE (validation) = 1.651763 for the model trained with rank = 3, lambda = 0.001, alpha = 0.0 and numIter = 10.\nRMSE (validation) = 1.651763 for the model trained with rank = 3, lambda = 0.001, alpha = 0.1 and numIter = 10.\nRMSE (validation) = 1.651763 for the model trained with rank = 3, lambda = 0.001, alpha = 0.3 and numIter = 10.\nRMSE (validation) = 1.651763 for the model trained with rank = 3, lambda = 0.001, alpha = 1.0 and numIter = 10.\nRMSE (validation) = 1.6133937 for the model trained with rank = 3, lambda = 0.01, alpha = 0.0 and numIter = 10.\nRMSE (validation) = 1.6133937 for the model trained with rank = 3, lambda = 0.01, alpha = 0.1 and numIter = 10.\nRMSE (validation) = 1.6133937 for the model trained with rank = 3, lambda = 0.01, alpha = 0.3 and numIter = 10.\nRMSE (validation) = 1.6133937 for the model trained with rank = 3, lambda = 0.01, alpha = 1.0 and numIter = 10.\nRMSE (validation) = 1.5750705 for the model trained with rank = 3, lambda = 0.1, alpha = 0.0 and numIter = 10.\nRMSE (validation) = 1.5750705 for the model trained with rank = 3, lambda = 0.1, alpha = 0.1 and numIter = 10.\nRMSE (validation) = 1.5750705 for the model trained with rank = 3, lambda = 0.1, alpha = 0.3 and numIter = 10.\nRMSE (validation) = 1.5750705 for the model trained with rank = 3, lambda = 0.1, alpha = 1.0 and numIter = 10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job 155 cancelled because SparkContext was shut down\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:816)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n  at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:816)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1685)\n  at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83)\n  at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1604)\n  at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1781)\n  at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1290)\n  at org.apache.spark.SparkContext.stop(SparkContext.scala:1780)\n  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:108)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913)\n  at org.apache.spark.rdd.RDD.count(RDD.scala:1134)\n  at org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:781)\n  at org.apache.spark.ml.recommendation.ALS.fit(ALS.scala:464)\n  at hyperParamSearchf$1(<console>:92)\n  at hyperParamSearch(<console>:108)\n  ... 50 elided\n"},"dateCreated":"2017-01-18T14:35:27+0000","dateStarted":"2017-01-18T15:42:14+0000","dateFinished":"2017-01-18T18:14:35+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:2893"},{"text":"","dateUpdated":"2017-01-18T18:14:39+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1484753914148_-1660638471","id":"20170118-153834_1838913238","dateCreated":"2017-01-18T15:38:34+0000","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:2894"}],"name":"marko_movieRec","id":"2C8U9ZE1A","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}